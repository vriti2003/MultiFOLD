# MultiFOLD
## A Multimodal Framework to correct OCR Lapses in cluttered Documents

Optical Character Recognition (OCR) systems achieve high accuracy on clean, printed text but struggle with cluttered or degraded documents, leading to frequent recognition errors. Existing correction methods either rely on labour-intensive manual proofreading or fail to incorporate contextual cues effectively. To address this, we introduce **MultiFOLD**, a multimodal framework that integrates **Automatic Speech Recognition (ASR)** and **context-aware refinement** for post-OCR correction. Unlike prior approaches that treat OCR and ASR outputs independently, MultiFOLD fuses textual and spoken corrections in a **confidence-weighted alignment strategy** and employs a fine-tuned {ByT5 sequence-to-sequence model} for enhanced text prediction. Our framework reduces the manual annotation burden by allowing users to verbally correct OCR errors, leveraging both **speech-based** and **text-based** correction mechanisms. Through extensive experiments performed on 800 cluttered documents, we demonstrate a **52.39\% reduction in annotation time** and a **36.04\% reduction in Character Error Rate (CER)** compared to manual OCR correction. MultiFOLD outperforms OCR-only and ASR-only baselines by a reduction in CER of 29.20\% and 6.81\%, respectively. These results establish MultiFOLD as an efficient, scalable, and open-source solution for document digitisation.

**Dataset:** Soon to be released publicly.
