{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c09b1fb63f45f386dba999c07938fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize processor and model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for OCR\n",
    "def ocr_image(src_img):\n",
    "    # Get the prediction from the OCR model\n",
    "    prompt = \"<|image|>Perform OCR on the image and extract the text string\"\n",
    "    inputs = processor(src_img, prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Generate output\n",
    "    output_ids = model.generate(**inputs,max_new_tokens=50)\n",
    "    output_text = processor.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return output_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the cluttering images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = '../sentence_images/cluttered'\n",
    "\n",
    "# looping through folders\n",
    "folder_list = os.listdir(parent_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page, idx, sentence = [], [], []\n",
    "\n",
    "for folder in folder_list :\n",
    "    files = os.listdir(os.path.join(parent_dir, folder))\n",
    "\n",
    "    img_files = [f for f in files if os.path.isfile(os.path.join(parent_dir, folder, f))]\n",
    "    for img in files:\n",
    "        page.append(folder)\n",
    "        idx.append(img[:-4])\n",
    "\n",
    "        full_image = Image.open(os.path.join(parent_dir, folder, img)).convert(\"RGB\")\n",
    "        text = ocr_image(full_image)\n",
    "        sentence.append(text)\n",
    "\n",
    "data = {\n",
    "        'page' : page,\n",
    "        'index' : idx,\n",
    "        'sentence' : sentence\n",
    "    }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"Llama_3_2_11B_vision_ocr.csv\", mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferencing for the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perform OCR on the image and extract the text string. <OCR/> to return his rammer, to present, prese nt, to to return his rammer, to present, to to to to to to to to to to to to to to to to to to to to to to to\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "path = '../sentence_images_/cluttered_/64/6.png'\n",
    "full_image = Image.open(path).convert(\"RGB\")\n",
    "text = ocr_image(full_image)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_models_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
